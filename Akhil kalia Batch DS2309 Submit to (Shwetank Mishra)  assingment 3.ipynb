{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fa397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_amazon_product(product_name):\n",
    "    base_url = \"https://www.amazon.in/s\"\n",
    "    params = {\n",
    "        \"k\": product_name,\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "\n",
    "    response = requests.get(base_url, params=params, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        product_listings = soup.find_all('div', class_='s-result-item')\n",
    "\n",
    "        if not product_listings:\n",
    "            print(\"No products found.\")\n",
    "            return\n",
    "\n",
    "        for index, product in enumerate(product_listings, start=1):\n",
    "            product_title = product.find('span', class_='a-text-normal')\n",
    "            product_price = product.find('span', class_='a-offscreen')\n",
    "\n",
    "            if product_title and product_price:\n",
    "                print(f\"{index}. {product_title.text.strip()} - {product_price.text.strip()}\")\n",
    "            elif product_title:\n",
    "                print(f\"{index}. {product_title.text.strip()} - Price not available\")\n",
    "    else:\n",
    "        print(f\"Error {response.status_code}: Unable to fetch the page.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = input(\"Enter the product you want to search on Amazon: \")\n",
    "    search_amazon_product(user_input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48731714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_product_details(product):\n",
    "    details = {\n",
    "        \"Brand Name\": \"-\",\n",
    "        \"Name of the Product\": \"-\",\n",
    "        \"Price\": \"-\",\n",
    "        \"Return/Exchange\": \"-\",\n",
    "        \"Expected Delivery\": \"-\",\n",
    "        \"Availability\": \"-\",\n",
    "        \"Product URL\": \"-\"\n",
    "    }\n",
    "\n",
    "    # Extract details here...\n",
    "\n",
    "    return details\n",
    "\n",
    "def search_amazon_products(product_name, max_pages=3):\n",
    "    # Search logic here...\n",
    "\n",
    "    return data\n",
    "\n",
    "def save_to_csv(data, filename=\"amazon_products.csv\"):\n",
    "    # Saving to CSV logic here...\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = input(\"Enter the product you want to search on Amazon: \")\n",
    "    search_results = search_amazon_products(user_input)\n",
    "    save_to_csv(search_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae42c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def save_image(keyword, image_url, index):\n",
    "    response = requests.get(image_url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(f\"{keyword}_image_{index}.jpg\", 'wb') as file:\n",
    "            for chunk in response.iter_content(1024):\n",
    "                file.write(chunk)\n",
    "        print(f\"Image {index} saved for {keyword}\")\n",
    "    else:\n",
    "        print(f\"Failed to download image {index} for {keyword}\")\n",
    "\n",
    "def scrape_images(keyword, num_images=10):\n",
    "    driver = webdriver.Chrome(executable_path='path/to/chromedriver')  # Set the path to your chromedriver executable\n",
    "\n",
    "    try:\n",
    "        driver.get(\"https://images.google.com/\")\n",
    "        time.sleep(2)  # Allow time for the page to load\n",
    "\n",
    "        search_bar = driver.find_element(\"name\", \"q\")\n",
    "        search_bar.clear()\n",
    "        search_bar.send_keys(keyword)\n",
    "        search_bar.send_keys(Keys.RETURN)\n",
    "\n",
    "        time.sleep(2)  # Allow time for search results to load\n",
    "\n",
    "        for i in range(1, num_images + 1):\n",
    "            try:\n",
    "                thumbnail = driver.find_element_by_xpath(f'//img[@data-index=\"{i-1}\"]')\n",
    "                thumbnail.click()\n",
    "                time.sleep(1)  # Allow time for the image to open\n",
    "\n",
    "                # Extract image URL from the modal\n",
    "                modal_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                image_url = modal_soup.find('img', class_='n3VNCb')['src']\n",
    "                \n",
    "                # Save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1b391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_smartphone_details(smartphone):\n",
    "    details = {\n",
    "        \"Brand Name\": \"-\",\n",
    "        \"Smartphone Name\": \"-\",\n",
    "        \"Colour\": \"-\",\n",
    "        \"RAM\": \"-\",\n",
    "        \"Storage(ROM)\": \"-\",\n",
    "        \"Primary Camera\": \"-\",\n",
    "        \"Secondary Camera\": \"-\",\n",
    "        \"Display Size\": \"-\",\n",
    "        \"Battery Capacity\": \"-\",\n",
    "        \"Price\": \"-\",\n",
    "        \"Product URL\": \"-\"\n",
    "    }\n",
    "\n",
    "    title = smartphone.find('div', class_='_4rR01T')\n",
    "    price = smartphone.find('div', class_='_30jeq3 _1_WHN1')\n",
    "    features = smartphone.find_all('li', class_='rgWa7D')\n",
    "\n",
    "    if title:\n",
    "        details[\"Smartphone Name\"] = title.text.strip()\n",
    "\n",
    "    if price:\n",
    "        details[\"Price\"] = price.text.strip()\n",
    "\n",
    "    for feature in features:\n",
    "        feature_text = feature.text.strip()\n",
    "        if \"RAM\" in feature_text:\n",
    "            details[\"RAM\"] = feature_text\n",
    "        elif \"Storage\" in feature_text:\n",
    "            details[\"Storage(ROM)\"] = feature_text\n",
    "        elif \"Primary Camera\" in feature_text:\n",
    "            details[\"Primary Camera\"] = feature_text\n",
    "        elif \"Secondary Camera\" in feature_text:\n",
    "            details[\"Secondary Camera\"] = feature_text\n",
    "        elif \"Display Size\" in feature_text:\n",
    "            details[\"Display Size\"] = feature_text\n",
    "        elif \"Battery Capacity\" in feature_text:\n",
    "            details[\"Battery Capacity\"] = feature_text\n",
    "\n",
    "    product_url = smartphone.find('a', class_='_1fQZEK')\n",
    "    if product_url:\n",
    "        details[\"Product URL\"] = product_url['href']\n",
    "\n",
    "    return details\n",
    "\n",
    "def search_flipkart_smartphones(keyword):\n",
    "    base_url = \"https://www.flipkart.com/search\"\n",
    "    params = {\n",
    "        \"q\": keyword\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        smartphone_listings = soup.find_all('div', class_='_1AtVbE')\n",
    "\n",
    "        if not smartphone_listings:\n",
    "            print(\"No smartphones found.\")\n",
    "            return []\n",
    "\n",
    "        smartphone_details = []\n",
    "        for smartphone in smartphone_listings:\n",
    "            details = get_smartphone_details(smartphone)\n",
    "            smartphone_details.append(details)\n",
    "\n",
    "        return smartphone_details\n",
    "    else:\n",
    "        print(f\"Error {response.status_code}: Unable to fetch the page.\")\n",
    "        return []\n",
    "\n",
    "def save_to_csv(data, filename=\"flipkart_smartphones.csv\"):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = input(\"Enter the smartphone you want to search on Flipkart: \")\n",
    "    search_results = search_flipkart_smartphones(user_input)\n",
    "    save_to_csv(search_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e0fd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_coordinates(api_key, city_name):\n",
    "    base_url = \"https://maps.googleapis.com/maps/api/geocode/json\"\n",
    "    params = {\n",
    "        \"address\": city_name,\n",
    "        \"key\": api_key,\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    if data['status'] == 'OK':\n",
    "        location = data['results'][0]['geometry']['location']\n",
    "        latitude = location['lat']\n",
    "        longitude = location['lng']\n",
    "        return latitude, longitude\n",
    "    else:\n",
    "        print(f\"Error: {data['status']}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    api_key = \"YOUR_API_KEY\"\n",
    "    city_name = input(\"Enter the city name: \")\n",
    "\n",
    "    coordinates = get_coordinates(api_key, city_name)\n",
    "\n",
    "    if coordinates:\n",
    "        print(f\"Coordinates for {city_name}: Latitude {coordinates[0]}, Longitude {coordinates[1]}\")\n",
    "    else:\n",
    "        print(\"Failed to retrieve coordinates.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff588e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_laptop_details(laptop):\n",
    "    details = {\n",
    "        \"Brand\": \"-\",\n",
    "        \"Model\": \"-\",\n",
    "        \"Processor\": \"-\",\n",
    "        \"RAM\": \"-\",\n",
    "        \"Storage\": \"-\",\n",
    "        \"Display Size\": \"-\",\n",
    "        \"Graphics Processor\": \"-\",\n",
    "        \"Price\": \"-\"\n",
    "    }\n",
    "\n",
    "    brand = laptop.find('div', class_='TopNumbeHeading sticky-footer')\n",
    "    model = laptop.find('div', class_='Product-specs-title')\n",
    "    specs = laptop.find_all('div', class_='value')\n",
    "\n",
    "    if brand:\n",
    "        details[\"Brand\"] = brand.text.strip()\n",
    "    if model:\n",
    "        details[\"Model\"] = model.text.strip()\n",
    "\n",
    "    for spec in specs:\n",
    "        text = spec.text.strip()\n",
    "        if \"Processor\" in text:\n",
    "            details[\"Processor\"] = text\n",
    "        elif \"RAM\" in text:\n",
    "            details[\"RAM\"] = text\n",
    "        elif \"Storage\" in text:\n",
    "            details[\"Storage\"] = text\n",
    "        elif \"Display Size\" in text:\n",
    "            details[\"Display Size\"] = text\n",
    "        elif \"Graphics Processor\" in text:\n",
    "            details[\"Graphics Processor\"] = text\n",
    "\n",
    "    price_tag = laptop.find('div', class_='Block-price')\n",
    "    if price_tag:\n",
    "        details[\"Price\"] = price_tag.text.strip()\n",
    "\n",
    "    return details\n",
    "\n",
    "def scrape_gaming_laptops():\n",
    "    base_url = \"https://www.digit.in/top-products/best-gaming-laptops-40.html\"\n",
    "    response = requests.get(base_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        laptops = soup.find_all('div', class_='TopNumbeHeading sticky-footer')\n",
    "\n",
    "        if not laptops:\n",
    "            print(\"No gaming laptops found.\")\n",
    "            return []\n",
    "\n",
    "        laptop_details = []\n",
    "        for laptop in laptops:\n",
    "            details = get_laptop_details(laptop)\n",
    "            laptop_details.append(details)\n",
    "\n",
    "        return laptop_details\n",
    "    else:\n",
    "        print(f\"Error {response.status_code}: Unable to fetch the page.\")\n",
    "        return []\n",
    "\n",
    "def save_to_csv(data, filename=\"gaming_laptops_details.csv\"):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    gaming_laptops_data = scrape_gaming_laptops()\n",
    "    \n",
    "    if gaming_laptops_data:\n",
    "        save_to_csv(gaming_laptops_data)\n",
    "    else:\n",
    "        print(\"No data to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41f5892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_billionaire_details(billionaire):\n",
    "    details = {\n",
    "        \"Rank\": \"-\",\n",
    "        \"Name\": \"-\",\n",
    "        \"Net Worth\": \"-\",\n",
    "        \"Age\": \"-\",\n",
    "        \"Citizenship\": \"-\",\n",
    "        \"Source\": \"-\",\n",
    "        \"Industry\": \"-\"\n",
    "    }\n",
    "\n",
    "    rank = billionaire.find('div', class_='rank')\n",
    "    name = billionaire.find('div', class_='personName')\n",
    "    net_worth = billionaire.find('div', class_='netWorth')\n",
    "    age = billionaire.find('div', class_='age')\n",
    "    citizenship = billionaire.find('div', class_='countryOfCitizenship')\n",
    "    source = billionaire.find('div', class_='source')\n",
    "    industry = billionaire.find('div',\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcb1b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import googleapiclient.discovery\n",
    "\n",
    "def get_authenticated_service(api_key):\n",
    "    youtube_api_service_name = \"youtube\"\n",
    "    youtube_api_version = \"v3\"\n",
    "    return googleapiclient.discovery.build(\n",
    "        youtube_api_service_name, youtube_api_version, developerKey=api_key)\n",
    "\n",
    "def get_video_comments(service, **kwargs):\n",
    "    comments = []\n",
    "    results = service.commentThreads().list(**kwargs).execute()\n",
    "\n",
    "    while results:\n",
    "        for item in results['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']\n",
    "            comments.append({\n",
    "                'text': comment['textDisplay'],\n",
    "                'author': comment['authorDisplayName'],\n",
    "                'votes': comment['likeCount'],\n",
    "                'published_at': comment['publishedAt']\n",
    "            })\n",
    "\n",
    "        # Get the next set of results\n",
    "        if 'nextPageToken' in results:\n",
    "            kwargs['pageToken'] = results['nextPageToken']\n",
    "            results = service.commentThreads().list(**kwargs).execute()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return comments\n",
    "\n",
    "def main(api_key, video_id):\n",
    "    service = get_authenticated_service(api_key)\n",
    "\n",
    "    comments = get_video_comments(\n",
    "        service,\n",
    "        part='snippet',\n",
    "        videoId=video_id,\n",
    "        textFormat='plainText',\n",
    "        maxResults=500\n",
    "    )\n",
    "\n",
    "    with open('youtube_comments.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(comments, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace 'YOUR_API_KEY' with your actual YouTube API key\n",
    "    api_key = 'YOUR_API_KEY'\n",
    "\n",
    "    # Replace 'VIDEO_ID' with the actual ID of the YouTube video\n",
    "    video_id = 'VIDEO_ID'\n",
    "\n",
    "    main(api_key, video_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168d76ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_hostel_details(hostel):\n",
    "    details = {\n",
    "        \"Hostel Name\": \"-\",\n",
    "        \"Distance from City Centre\": \"-\",\n",
    "        \"Ratings\": \"-\",\n",
    "        \"Total Reviews\": \"-\",\n",
    "        \"Overall Reviews\": \"-\",\n",
    "        \"Privates From Price\": \"-\",\n",
    "        \"Dorms From Price\": \"-\",\n",
    "        \"Facilities\": \"-\",\n",
    "        \"Property Description\": \"-\"\n",
    "    }\n",
    "\n",
    "    name = hostel.find('h2', class_='title-2')\n",
    "    distance = hostel.find('span', class_='description')\n",
    "    ratings = hostel.find('div', class_='score orange big')\n",
    "    total_reviews = hostel.find('div', class_='reviews')\n",
    "    overall_reviews = hostel.find('div', class_='keyword')\n",
    "    privates_price = hostel.find('div', class_='price-col privates from')\n",
    "    dorms_price = hostel.find('div', class_='price-col dorms from')\n",
    "    facilities = hostel.find('ul', class_='facilities').text.strip()\n",
    "    description = hostel.find('div', class_='rating-factors prop-card-tablet rating-factors small').text.strip()\n",
    "\n",
    "    if name:\n",
    "        details[\"Hostel Name\"] = name.text.strip()\n",
    "\n",
    "    if distance:\n",
    "        details[\"Distance from City Centre\"] = distance.text.strip()\n",
    "\n",
    "    if ratings:\n",
    "        details[\"Ratings\"] = ratings.text.strip()\n",
    "\n",
    "    if total_reviews:\n",
    "        details[\"Total Reviews\"] = total_reviews.text.strip()\n",
    "\n",
    "    if overall_reviews:\n",
    "        details[\"Overall Reviews\"] = overall_reviews.text.strip()\n",
    "\n",
    "    if privates_price:\n",
    "        details[\"Privates From Price\"] = privates_price.text.strip()\n",
    "\n",
    "    if dorms_price:\n",
    "        details[\"Dorms From Price\"] = dorms_price.text.strip()\n",
    "\n",
    "    details[\"Facilities\"] = facilities\n",
    "\n",
    "    if description:\n",
    "        details[\"Property Description\"] = description\n",
    "\n",
    "    return details\n",
    "\n",
    "def scrape_hostels_in_london():\n",
    "    base_url = \"https://www.hostelworld.com/s?q=London,%20England&city=London&country=England&from=2023-11-01&to=2023-11-10&guests=1&page=1&results=240\"\n",
    "    response = requests.get(base_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        hostels = soup.find_all('div', class_='property-card')\n",
    "\n",
    "        if not hostels:\n",
    "            print(\"No hostels found.\")\n",
    "            return []\n",
    "\n",
    "        hostel_details = []\n",
    "        for hostel in hostels:\n",
    "            details = get_hostel_details(hostel)\n",
    "            hostel_details.append(details)\n",
    "\n",
    "        return hostel_details\n",
    "    else:\n",
    "        print(f\"Error {response.status_code}: Unable to fetch the page.\")\n",
    "        return []\n",
    "\n",
    "def save_to_csv(data, filename=\"hostels_in_london.csv\"):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hostels_data = scrape_hostels_in_london()\n",
    "\n",
    "    if hostels_data:\n",
    "        save_to_csv(hostels_data)\n",
    "    else:\n",
    "        print(\"No data to save.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17d4cae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
