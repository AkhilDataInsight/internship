{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feea7c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Header Text\n",
      "0  \\n\\nWikipedia\\n\\nThe Free Encyclopedia\\n\n",
      "1  \\n\\n\\n\\n1 000 000+\\n\\n\\narticles\\n\\n\\n\\n\n",
      "2    \\n\\n\\n\\n100 000+\\n\\n\\narticles\\n\\n\\n\\n\n",
      "3     \\n\\n\\n\\n10 000+\\n\\n\\narticles\\n\\n\\n\\n\n",
      "4      \\n\\n\\n\\n1 000+\\n\\n\\narticles\\n\\n\\n\\n\n",
      "5        \\n\\n\\n\\n100+\\n\\n\\narticles\\n\\n\\n\\n\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the webpage\n",
    "url = \"https://www.wikipedia.org\"\n",
    "\n",
    "# Send an HTTP GET request to the URL and get the webpage content\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find all header tags (h1, h2, h3, h4, h5, h6) on the page\n",
    "header_tags = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "\n",
    "# Extract the text from each header tag and store it in a list\n",
    "header_texts = [tag.text for tag in header_tags]\n",
    "\n",
    "# Create a DataFrame to store the header text\n",
    "df = pd.DataFrame({'Header Text': header_texts})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50cadf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI Teams:\n",
      "  Rank              Team Matches Points Rating\n",
      "0    1        India\\nIND      49  5,839    119\n",
      "1    2    Australia\\nAUS      36  4,015    112\n",
      "2    3     Pakistan\\nPAK      32  3,525    110\n",
      "3    4  South Africa\\nSA      29  3,166    109\n",
      "4    5   New Zealand\\nNZ      38  4,007    105\n",
      "5    6      England\\nENG      34  3,377     99\n",
      "6    7     Sri Lanka\\nSL      43  3,943     92\n",
      "7    8   Bangladesh\\nBAN      40  3,574     89\n",
      "8    9  Afghanistan\\nAFG      26  2,170     83\n",
      "9   10   West Indies\\nWI      38  2,582     68\n",
      "\n",
      "Top 10 ODI Batsmen:\n",
      "                                                Rank                   Team  \\\n",
      "0               1\\n                        \\n\\n\\n(0)             Babar Azam   \n",
      "1       2\\n                                \\n\\n\\n(0)           Shubman Gill   \n",
      "2  3\\n                                \\n\\n\\n\\n\\n(...        Quinton de Kock   \n",
      "3  4\\n                                \\n\\n\\n\\n\\n(...       Heinrich Klaasen   \n",
      "4  5\\n                                \\n\\n\\n\\n\\n(...           David Warner   \n",
      "5  =\\n                                \\n\\n\\n\\n\\n(...            Virat Kohli   \n",
      "6       7\\n                                \\n\\n\\n(0)           Harry Tector   \n",
      "7  8\\n                                \\n\\n\\n\\n\\n(...           Rohit Sharma   \n",
      "8  9\\n                                \\n\\n\\n\\n\\n(...  Rassie van der Dussen   \n",
      "9      10\\n                                \\n\\n\\n(0)            Imam-ul-Haq   \n",
      "\n",
      "  Matches Points                         Rating  \n",
      "0     PAK    829  898 v West Indies, 10/06/2022  \n",
      "1     IND    823    847 v Australia, 24/09/2023  \n",
      "2      SA    769    813 v Sri Lanka, 10/03/2019  \n",
      "3      SA    756   756 v Bangladesh, 24/10/2023  \n",
      "4     AUS    747     880 v Pakistan, 26/01/2017  \n",
      "5     IND    747      911 v England, 12/07/2018  \n",
      "6     IRE    729      729 v England, 23/09/2023  \n",
      "7     IND    725    885 v Sri Lanka, 06/07/2019  \n",
      "8      SA    716      796 v England, 19/07/2022  \n",
      "9     PAK    704  815 v West Indies, 12/06/2022  \n",
      "\n",
      "Top 10 ODI Bowlers:\n",
      "                                                Rank            Team Matches  \\\n",
      "0               1\\n                        \\n\\n\\n(0)  Josh Hazlewood     AUS   \n",
      "1       2\\n                                \\n\\n\\n(0)  Mohammed Siraj     IND   \n",
      "2  3\\n                                \\n\\n\\n\\n\\n(...  Keshav Maharaj      SA   \n",
      "3  4\\n                                \\n\\n\\n\\n\\n(...     Rashid Khan     AFG   \n",
      "4  5\\n                                \\n\\n\\n\\n\\n(...     Trent Boult      NZ   \n",
      "5  6\\n                                \\n\\n\\n\\n\\n(...   Mohammad Nabi     AFG   \n",
      "6  7\\n                                \\n\\n\\n\\n\\n(...      Adam Zampa     AUS   \n",
      "7  8\\n                                \\n\\n\\n\\n\\n(...      Matt Henry      NZ   \n",
      "8       9\\n                                \\n\\n\\n(0)   Kuldeep Yadav     IND   \n",
      "9      10\\n                                \\n\\n\\n(0)  Shaheen Afridi     PAK   \n",
      "\n",
      "  Points                          Rating  \n",
      "0    670       733 v England, 26/01/2018  \n",
      "1    668   736 v New Zealand, 21/01/2023  \n",
      "2    656    656 v Bangladesh, 24/10/2023  \n",
      "3    654      806 v Pakistan, 21/09/2018  \n",
      "4    653     775 v Australia, 11/09/2022  \n",
      "5    641      657 v Zimbabwe, 09/06/2022  \n",
      "6    635  670 v South Africa, 09/09/2023  \n",
      "7    634    691 v Bangladesh, 26/03/2021  \n",
      "8    632   765 v New Zealand, 26/01/2019  \n",
      "9    625   688 v West Indies, 10/06/2022  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape and create a DataFrame\n",
    "def scrape_icc_rankings(url, headers):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the table containing the rankings data\n",
    "    table = soup.find('table')\n",
    "\n",
    "    # Extract the data from the table\n",
    "    data = []\n",
    "    for row in table.find_all('tr')[1:11]:  # Top 10\n",
    "        columns = row.find_all('td')\n",
    "        values = [column.text.strip() for column in columns]\n",
    "        data.append(values)\n",
    "\n",
    "    # Create a DataFrame with the extracted data\n",
    "    df = pd.DataFrame(data, columns=['Rank', 'Team', 'Matches', 'Points', 'Rating'])\n",
    "\n",
    "    return df\n",
    "\n",
    "# URL and headers for the ODI teams rankings\n",
    "url_teams = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Scrape and create a DataFrame for the top 10 ODI teams\n",
    "df_teams = scrape_icc_rankings(url_teams, headers)\n",
    "\n",
    "# URL and headers for the ODI batsmen rankings\n",
    "url_batsmen = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\"\n",
    "# Scrape and create a DataFrame for the top 10 ODI batsmen\n",
    "df_batsmen = scrape_icc_rankings(url_batsmen, headers)\n",
    "\n",
    "# URL and headers for the ODI bowlers rankings\n",
    "url_bowlers = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\"\n",
    "# Scrape and create a DataFrame for the top 10 ODI bowlers\n",
    "df_bowlers = scrape_icc_rankings(url_bowlers, headers)\n",
    "\n",
    "# Display the DataFrames\n",
    "print(\"Top 10 ODI Teams:\")\n",
    "print(df_teams)\n",
    "print(\"\\nTop 10 ODI Batsmen:\")\n",
    "print(df_batsmen)\n",
    "print(\"\\nTop 10 ODI Bowlers:\")\n",
    "print(df_bowlers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "826a3eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Women's ODI Teams:\n",
      "  Rank              Team Matches Points Rating\n",
      "0    1    Australia\\nAUS      19  3,084    162\n",
      "1    2      England\\nENG      23  2,991    130\n",
      "2    3  South Africa\\nSA      21  2,446    116\n",
      "3    4        India\\nIND      18  1,745     97\n",
      "4    5   New Zealand\\nNZ      21  2,014     96\n",
      "5    6   West Indies\\nWI      18  1,610     89\n",
      "6    7     Sri Lanka\\nSL       9    714     79\n",
      "7    8   Bangladesh\\nBAN      11    816     74\n",
      "8    9     Thailand\\nTHA      11    753     68\n",
      "9   10     Pakistan\\nPAK      21  1,435     68\n",
      "\n",
      "Top 10 Women's ODI Batting Players:\n",
      "                                            Rank                  Team  \\\n",
      "0           1\\n                        \\n\\n\\n(0)  Natalie Sciver-Brunt   \n",
      "1   2\\n                                \\n\\n\\n(0)           Beth Mooney   \n",
      "2   3\\n                                \\n\\n\\n(0)   Chamari Athapaththu   \n",
      "3   4\\n                                \\n\\n\\n(0)       Laura Wolvaardt   \n",
      "4   5\\n                                \\n\\n\\n(0)       Smriti Mandhana   \n",
      "5   6\\n                                \\n\\n\\n(0)          Alyssa Healy   \n",
      "6   7\\n                                \\n\\n\\n(0)          Ellyse Perry   \n",
      "7   8\\n                                \\n\\n\\n(0)      Harmanpreet Kaur   \n",
      "8   9\\n                                \\n\\n\\n(0)           Meg Lanning   \n",
      "9  10\\n                                \\n\\n\\n(0)        Marizanne Kapp   \n",
      "\n",
      "  Matches Points                         Rating  \n",
      "0     ENG    807    807 v Sri Lanka, 14/09/2023  \n",
      "1     AUS    750      776 v England, 12/07/2023  \n",
      "2      SL    736  758 v New Zealand, 03/07/2023  \n",
      "3      SA    727    741 v Australia, 22/03/2022  \n",
      "4     IND    708      797 v England, 28/02/2019  \n",
      "5     AUS    698      785 v England, 03/04/2022  \n",
      "6     AUS    697  766 v West Indies, 11/09/2019  \n",
      "7     IND    694      731 v England, 21/09/2022  \n",
      "8     AUS    662  834 v New Zealand, 24/02/2016  \n",
      "9      SA    642  642 v New Zealand, 01/10/2023  \n",
      "\n",
      "Top 10 Women's ODI All-Rounders:\n",
      "                                            Rank                  Team  \\\n",
      "0           1\\n                        \\n\\n\\n(0)        Marizanne Kapp   \n",
      "1   2\\n                                \\n\\n\\n(0)      Ashleigh Gardner   \n",
      "2   3\\n                                \\n\\n\\n(0)  Natalie Sciver-Brunt   \n",
      "3   4\\n                                \\n\\n\\n(0)       Hayley Matthews   \n",
      "4   5\\n                                \\n\\n\\n(0)           Amelia Kerr   \n",
      "5   6\\n                                \\n\\n\\n(0)         Deepti Sharma   \n",
      "6   7\\n                                \\n\\n\\n(0)          Ellyse Perry   \n",
      "7   8\\n                                \\n\\n\\n(0)         Jess Jonassen   \n",
      "8   =\\n                                \\n\\n\\n(0)         Sophie Devine   \n",
      "9  10\\n                                \\n\\n\\n(0)              Nida Dar   \n",
      "\n",
      "  Matches Points                          Rating  \n",
      "0      SA    385   419 v West Indies, 10/09/2021  \n",
      "1     AUS    377   391 v West Indies, 08/10/2023  \n",
      "2     ENG    360     421 v Australia, 18/07/2023  \n",
      "3      WI    358       392 v Ireland, 26/06/2023  \n",
      "4      NZ    346   356 v West Indies, 25/09/2022  \n",
      "5     IND    312  397 v South Africa, 09/10/2019  \n",
      "6     AUS    282   548 v West Indies, 11/09/2019  \n",
      "7     AUS    227   308 v West Indies, 11/09/2019  \n",
      "8      NZ    227     305 v Australia, 05/10/2020  \n",
      "9     PAK    224     232 v Australia, 21/01/2023  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape and create a DataFrame\n",
    "def scrape_icc_rankings(url, headers):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the table containing the rankings data\n",
    "    table = soup.find('table')\n",
    "\n",
    "    # Extract the data from the table\n",
    "    data = []\n",
    "    for row in table.find_all('tr')[1:11]:  # Top 10\n",
    "        columns = row.find_all('td')\n",
    "        values = [column.text.strip() for column in columns]\n",
    "        data.append(values)\n",
    "\n",
    "    # Create a DataFrame with the extracted data\n",
    "    df = pd.DataFrame(data, columns=['Rank', 'Team', 'Matches', 'Points', 'Rating'])\n",
    "\n",
    "    return df\n",
    "\n",
    "# URL and headers for the Women's ODI teams rankings\n",
    "url_teams_women = \"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Scrape and create a DataFrame for the top 10 Women's ODI teams\n",
    "df_teams_women = scrape_icc_rankings(url_teams_women, headers)\n",
    "\n",
    "# URL and headers for the Women's ODI batting players rankings\n",
    "url_batting_women = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\"\n",
    "# Scrape and create a DataFrame for the top 10 Women's ODI batting players\n",
    "df_batting_women = scrape_icc_rankings(url_batting_women, headers)\n",
    "\n",
    "# URL and headers for the Women's ODI all-rounders rankings\n",
    "url_allrounders_women = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\"\n",
    "# Scrape and create a DataFrame for the top 10 Women's ODI all-rounders\n",
    "df_allrounders_women = scrape_icc_rankings(url_allrounders_women, headers)\n",
    "\n",
    "# Display the DataFrames\n",
    "print(\"Top 10 Women's ODI Teams:\")\n",
    "print(df_teams_women)\n",
    "print(\"\\nTop 10 Women's ODI Batting Players:\")\n",
    "print(df_batting_women)\n",
    "print(\"\\nTop 10 Women's ODI All-Rounders:\")\n",
    "print(df_allrounders_women)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50d18880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Headline, Time, News Link]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the CNBC world news page\n",
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "\n",
    "# Send an HTTP GET request to the URL and get the webpage content\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find all the news articles on the page\n",
    "articles = soup.find_all('div', class_='CardHeadlines-story')\n",
    "\n",
    "# Extract data (Headline, Time, and News Link) from the articles\n",
    "data = []\n",
    "for article in articles:\n",
    "    headline = article.find('a', class_='CardHeadlines-headline').text.strip()\n",
    "    time = article.find('time', class_='CardHeadlines-time').text.strip()\n",
    "    news_link = article.find('a', class_='CardHeadlines-headline')['href']\n",
    "    data.append((headline, time, news_link))\n",
    "\n",
    "# Create a DataFrame to store the extracted data\n",
    "df = pd.DataFrame(data, columns=['Headline', 'Time', 'News Link'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "276a4c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Paper Title, Authors, Published Date, Paper URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the most downloaded articles page\n",
    "url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "\n",
    "# Send an HTTP GET request to the URL and get the webpage content\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find all the articles on the page\n",
    "articles = soup.find_all('div', class_='article-content')\n",
    "\n",
    "# Extract data (Paper Title, Authors, Published Date, and Paper URL) from the articles\n",
    "data = []\n",
    "for article in articles:\n",
    "    title = article.find('a', class_='text-m').text.strip()\n",
    "    authors = article.find('div', class_='authors').text.strip()\n",
    "    date = article.find('div', class_='date').text.strip()\n",
    "    paper_url = \"https://www.journals.elsevier.com\" + article.find('a', class_='text-m')['href']\n",
    "    data.append((title, authors, date, paper_url))\n",
    "\n",
    "# Create a DataFrame to store the extracted data\n",
    "df = pd.DataFrame(data, columns=['Paper Title', 'Authors', 'Published Date', 'Paper URL'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a7f921d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Restaurant Name, Cuisine, Location, Ratings, Image URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the dineout.co.in page with restaurant details\n",
    "url = \"https://www.dineout.co.in/bangalore-restaurants\"\n",
    "\n",
    "# Send an HTTP GET request to the URL and get the webpage content\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find all the restaurant cards on the page\n",
    "restaurant_cards = soup.find_all('div', class_='restnt-card-main-div')\n",
    "\n",
    "# Extract data (Restaurant Name, Cuisine, Location, Ratings, and Image URL) from the restaurant cards\n",
    "data = []\n",
    "for card in restaurant_cards:\n",
    "    restaurant_name = card.find('div', class_='restnt-card-main-title').text.strip()\n",
    "    cuisine = card.find('div', class_='restnt-card-main-cuisine').text.strip()\n",
    "    location = card.find('div', class_='restnt-card-main-location').text.strip()\n",
    "    ratings = card.find('span', class_='dv-rating-num').text.strip()\n",
    "    image_url = card.find('img', class_='no-bg-logo')['data-src']\n",
    "    data.append((restaurant_name, cuisine, location, ratings, image_url))\n",
    "\n",
    "# Create a DataFrame to store the extracted data\n",
    "df = pd.DataFrame(data, columns=['Restaurant Name', 'Cuisine', 'Location', 'Ratings', 'Image URL'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fa416a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in ./anaconda3/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./anaconda3/lib/python3.11/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./anaconda3/lib/python3.11/site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.11/site-packages (from requests) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0151983f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in ./anaconda3/lib/python3.11/site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./anaconda3/lib/python3.11/site-packages (from beautifulsoup4) (2.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acd6d892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./anaconda3/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in ./anaconda3/lib/python3.11/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in ./anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c71fef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve the website content.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL of the website to scrape\n",
    "url = \"https://presidentofindia.nic.in/former-presidents.htm\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the table containing the list of former presidents\n",
    "    table = soup.find('table', class_='table')\n",
    "\n",
    "    # Initialize lists to store the data\n",
    "    names = []\n",
    "    terms = []\n",
    "\n",
    "    # Iterate through the rows of the table\n",
    "    for row in table.find_all('tr')[1:]:  # Skip the header row\n",
    "        columns = row.find_all('td')\n",
    "        if len(columns) >= 2:\n",
    "            name = columns[0].text.strip()\n",
    "            term = columns[1].text.strip()\n",
    "            names.append(name)\n",
    "            terms.append(term)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    data = {'Name': names, 'Term of Office': terms}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(df)\n",
    "\n",
    "else:\n",
    "    print(\"Failed to retrieve the website content.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d66493a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
